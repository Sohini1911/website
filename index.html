<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ekdeep Singh Lubana</title>
  
  <meta name="author" content="Ekdeep Singh Lubana">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ekdeep Singh Lubana</name>
              </p>

        <!-- Description -->
              <p>I am a research fellow at the CBS-NTT Program in Physics of Intelligence at Harvard University, leading the phenomenological theory team and often collaborating with <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>, <a href="https://davidscottkrueger.com">David Krueger</a>, and <a href="https://www.demba-ba.org/">Demba Ba</a>. I did my PhD co-affiliated with EECS, University of Michigan and CBS, Harvard, and was advised by <a href="http://robertdick.org/">Robert Dick</a> and <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>.
              </p>
              <p>
                I am generally interested in designing (faithful) abstractions of phenomena relevant to controlling or aligning neural networks. I am also very interested in better understanding training dynamics of neural networks, especially via a statistical physics perspective. 
              </p>
              <p>
                I graduated with a Bachelor's degree in ECE from Indian Institute of Technology (IIT), Roorkee in 2019. My research in undergraduate was primarily focused on embedded systems, such as energy-efficient machine vision systems.
              </p>

        <!-- Stuff -->
              <p style="text-align:center">
                <a href="mailto:ekdeeplubana@fas.harvard.edu">Email</a> &nbsp/&nbsp
                <a href="data/ekdeep-cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=OP7S3vsAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/EkdeepSLubana">Github</a>
              </p>
            </td>

        <!-- Front image -->
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ekdeep.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ekdeep.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <!-- News -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr> <td width="100%" valign="top">
        <heading>News</heading>
        <table width="100%" valign="top" border="0" cellspacing="0" cellpadding="2">

        <tr valign="top"> <td> [12/2024] </td>  <td> 
        Paper identifying <a href="https://arxiv.org/abs/2410.11767">(in)abilities of SAEs</a> awarded <font color="red">best paper</font> at NeurIPS Foundation model interventions workshop!
        </td> </tr>

          
        <tr valign="top"> <td> [09/2024] </td>  <td> 
        Paper on <a href="https://arxiv.org/abs/2406.19370">hidden capabilities in generative models</a> accepted as a <font color="red">spotlight</font> at NeurIPS, 2024.
        </td> </tr>

          
        <tr valign="top"> <td> [08/2024] </td>  <td> 
        Preprint on <a href="https://arxiv.org/abs/2408.12578">a percolation model of emergent capabilities</a> is on arXiv now. 
        </td> </tr>

          
        <tr valign="top"> <td> [06/2024] </td>  <td> 
        Paper on <a href="data/sftjailbreaks.pdf">identifying how jailbreaks bypass safety mechanisms</a> accepted at NeurIPS 2024.
        </td> </tr>

          
        <tr valign="top"> <td> [10/2023] </td>  <td> 
        Paper on <a href="https://arxiv.org/abs/2310.17639">analyzing in-context learning as a subjective randomness task</a> accepted to ICLR, 2024. 
        </td> </tr>

          
        <tr valign="top"> <td> [10/2023] </td>  <td> 
        Our work on <a href="https://arxiv.org/abs/2310.09336">multiplicative emergence of compositional abilities</a> was accepted to NeurIPS, 2023. 
        </td> </tr>

          
        <tr valign="top"> <td> [04/2023] </td>  <td> 
        Our work on <a href="https://arxiv.org/abs/2211.08422">a mechanistic understanding of loss landscapes</a> was accepted to ICML, 2023.  
        </td> </tr>

          
        <tr valign="top"> <td> [01/2023] </td>  <td> 
        Our work analyzing <a href="https://arxiv.org/pdf/2210.00638.pdf">loss landscape of self-supervised objectives</a> was accepted to ICLR, 2023.  
        </td> </tr>

          
<!--         <tr valign="top"> <td> [05/2022] </td>  <td> 
        Our work on <a href="https://arxiv.org/abs/2205.11506">unsupervised federated learning</a> was accepted as a <font color="red">spotlight</font> at ICML, 2022.
        </td> </tr>

 -->
        <tr valign="top"> <td> [10/2021] </td>  <td> 
        Our work on <a href="https://arxiv.org/abs/2106.05956">dynamics of normalization layers</a> was accepted to NeurIPS, 2021.  
        </td> </tr>

          
          <tr valign="top"> <td> [03/2021] </td>  <td> 
        Our work on <a href="https://openreview.net/forum?id=rumv7QmLUue">theory of pruning</a> was accepted as a <font color="red">spotlight</font> at ICLR, 2021.  
        </td> </tr>

        </table>
        </td>  </tr> </tbody></table>


        <!-- Publications -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications (* denotes equal contribution)</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/unified_icl.png" alt="Rationally doing ICL" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2506.17859">
                <papertitle>In-Context Learning Strategies Emerge Rationally</papertitle>
              </a>
              <br>
              <a href="https://ca.linkedin.com/in/daniel-wurgaft-4841b41b8">Daniel Wurgaft*</a>,
              <strong>Ekdeep Singh Lubana*</strong>,
              <a href="https://cfpark00.github.io/">Core Francisco Park</a>,
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>, <br>
              <a href="https://reddylab.physics.princeton.edu/">Gautam Reddy</a>, and
              <a href="https://cocolab.stanford.edu/ndg">Noah Goodman</a>
              <br>
              <em>Preprint</em>, 2025
              <br> 
              <p>We model ICL in a hierarchical Bayesian framework as a posterior-weighted average of two well-defined algorithmic strategies. Taking a rational analysis lens, we show across three popular setttings, that we can analytically predict model behavior and characterize the algorithmic phase-diagram identified in ICL in our prior work.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/blindspots.png" alt="Conceptual Blindspots" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2506.19708">
                <papertitle>Uncovering Conceptual Blindspots in Generative Image Models Using Sparse Autoencoders</papertitle>
              </a>
              <br>
              <a href="https://www.matyasbohacek.com/">Matyas Bohacek*</a>,
              <a href="https://thomasfel.fr/">Thomas Fel*</a>,
              <a href="https://graphics.stanford.edu/~maneesh/">Maneesh Agrawala</a>, and
              <strong>Ekdeep Singh Lubana</strong>
              <br>
              <em>Preprint</em>, 2025
              <br> 
              <p>We build on results from identifiability theory to formalize a measure that assesses fine-grained differences in a natural distribution and a generative model trained upon it.</p>
            </td>
          </tr>

          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mpsae.png" alt="MP-SAEs" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2506.03093">
                <papertitle>From Flat to Hierarchical: Extracting Sparse Representations with Matching Pursuit</papertitle>
              </a>
              <br>
              <a href="https://people.epfl.ch/valerie.costa?lang=en">Valerie Costa*</a>,
              <a href="https://thomasfel.fr/">Thomas Fel*</a>,
              <strong>Ekdeep Singh Lubana*</strong>,
              <a href="https://btolooshams.github.io/">Bahareh Tolooshams</a>, and
              <a href="demba-ba.org">Demba Ba</a>
              <br>
              <em>Preprint</em>, 2025
              <br> 
              <p>We use recent phenomenology of neural network representations, e.g., their ability to encode hierarchical and nonlinear concepts, to argue for the limitations of Linear representation hypothesis, contextualizing SAEs with respect to such concepts.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/projecting_assumptions.png" alt="SAEs and Data" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2503.01822">
                <papertitle>Projecting Assumptions: The Duality Between Sparse Autoencoders and Concept Geometry</papertitle>
              </a>
              <br>
              <a href="https://sites.google.com/view/saisumedh">Sai Sumedh R. Hindupur*</a>,
              <strong>Ekdeep Singh Lubana*</strong>,
              <a href="https://thomasfel.fr/">Thomas Fel*</a>, and
              <a href="demba-ba.org">Demba Ba</a>              
              <br>
              <em>Preprint</em>, 2025
              <br> 
              <p>We demonstrate a duality between how concepts are organized in model representations and the optimal SAE that will help identify them. This implies there exists no universally optimal SAE that can be used across models and domains.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/archetypal.png" alt="Archetypal SAE" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2502.12892">
                <papertitle>Archetypal SAE: Adaptive and Stable Dictionary Learning for Concept Extraction in Large Vision Models</papertitle>
              </a>
              <br>
              <a href="https://thomasfel.fr/">Thomas Fel*</a>,
              <strong>Ekdeep Singh Lubana*</strong>,
              <a href="https://jacob-prince.github.io/">Jacob S. Prince</a>, 
              <a href="https://mkowal2.github.io/">Matthew Kowal</a>,
              <a href="https://victorboutin.github.io/">Victor Boutin</a>,
              <a href="https://www.isabelpapad.com/">Isabel Papadimitriou</a>,
              <a href="https://scholar.harvard.edu/binxuw/home">Binxu Wang</a>,
              <a href="https://www.bewitched.com/">Martin Wattenberg</a>,
              <a href="demba-ba.org">Demba Ba</a>, and
              <a href="https://konklab.fas.harvard.edu/">Talia Konkle</a>
              
              <br>
              <em>International Conference on Machine Learning (ICML)</em>, 2025
              <br> 
              <p>We use vision models to demonstrate algorithmic instability in SAEs, i.e., different runs of the same pipeline yield different features / model interpretations. We propose a geometrical constraint to substantially address this issue.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iclriclr.png" alt="ICLR-ICLR" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2501.00070">
                <papertitle>ICLR: In-Context Learning of Representations</papertitle>
              </a>
              <br>
              <a href="https://cfpark00.github.io/">Core Francisco Park*</a>,
              <a href="https://ajyl.github.io/about">Andrew Lee*</a>,
              <strong>Ekdeep Singh Lubana*</strong>,
              <a href="https://fftyyy.github.io/">Yongyi Yang*</a>,<br>
              <a href="https://kentonishi.com">Kento Nishi</a>,
              <a href="https://www.linkedin.com/in/maya-okawa">Maya Okawa</a>, 
              <a href="https://www.bewitched.com/">Martin Wattenberg</a>, and
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2025
              <br> 
              <p>We demonstrate models can in-context infer novel semantics of a concept, overriding their original meaning based on pretraining. This process occurs relatively rapidly, yielding an emergent reorganization of individual concepts' representations.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/markovicl.png" alt="MarkovICL" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2412.01003">
                <papertitle>Competition Dynamics Shape Algorithmic Phases of In-Context Learning</papertitle>
              </a>
              <br>
              <a href="https://cfpark00.github.io/">Core Francisco Park*</a>,
              <strong>Ekdeep Singh Lubana*</strong>,
              <a href="https://www.linkedin.com/in/itamar-pres-3a56a4b3">Itamar Pres</a>, and
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2025 <font color="red"> (Spotlight)</font> 
              <br> 
              <p>We demonstrate an algorithmic phase diagram that decomposes ICL into a mixture of algorithms, showing that phenomenology of ICL may be specific to setups used for experimentation.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sae.png" alt="SAEs and Formal Languages" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2410.11767">
                <papertitle>Analyzing (In)Abilities of SAEs via Formal Languages</papertitle>
              </a>
              <br>
              <a href="https://in.linkedin.com/in/abhinav-menon-833404229">Abhinav Menon*</a>,
              <a href="https://www.iiit.ac.in/faculty/manish-shrivastava/">Manish Srivastava</a>,
              <a href="https://davidscottkrueger.com">David Krueger</a>, and
              <strong>Ekdeep Singh Lubana*</strong>
              <br>
              <em>Proceedings of NAACL</em>, 2025 <font color="red"> (Oral)</font> 
              <br>
              <em>NeurIPS workshop on Foundation Model Interventions </em>, 2024 <font color="red"> (Awarded Best Paper)</font> 
              <br> 
<!--               <a href="data/sae.bib">bibtex</a> / <a href="https://arxiv.org/abs/2410.11767">arXiv</a> -->
              <p>We use Formal languages to analyze the limitations of SAEs, finding, similar to prior work in disentangled representation learning, that SAEs find correlational features; explicit biasing is necessary to induce causality.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/percolation.png" alt="Percolation Model of Emergence" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2408.12578">
                <papertitle>A Percolation Model of Emergence: Analyzing Transformers Trained on a Formal Language</papertitle>
              </a>
              <br>
              <strong>Ekdeep Singh Lubana*</strong>,
              <a href="https://www.bdr.riken.jp/en/research/labs/kawaguchi-k/index.html">Kyogo Kawaguchi*</a>,
              <a href="http://robertdick.org/">Robert P. Dick</a>, and
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2025
              <br> 
<!--               <a href="data/percolation.bib">bibtex</a> / <a href="https://arxiv.org/abs/2408.12578">arXiv</a> -->
              <p>We implicate rapid acquisition of structures underlying the data generating process as the source of sudden learning of capabilities, and analogize knowledge centric capabilities to the process of graph percolation that undergoes a formal second-order phase transition.</p>
            </td>
          </tr>

          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/conceptdynamics.png" alt="Concept learning" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2410.08309">
                <papertitle>Dynamics of Concept Learning and Compositional Generalization</papertitle>
              </a>
              <br>
              <a href="https://fftyyy.github.io/">Yongyi Yang</a>,
              <a href="https://cfpark00.github.io/">Core Francisco Park</a>,
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="https://www.linkedin.com/in/maya-okawa">Maya Okawa</a>,
              <a href="http://weihu.me">Wei Hu</a>, and
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2025
              <br> 
<!--               <a href="data/conceptdynamics.bib">bibtex</a> / <a href="https://arxiv.org/abs/2410.08039">arXiv</a> -->
              <p>We create a theoretical abstraction of our prior work on compositional generalization and justify the peculiar learning dynamics observed therein, finding there was in fact a quadruple descent embedded therein!</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/shattering.png" alt="Representation Shattering" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2410.17194">
                <papertitle>Representation Shattering in Transformers: A Synthetic Study with Knowledge Editing</papertitle>
              </a>
              <br>
              <a href="https://kentonishi.com">Kento Nishi</a>,
              <a href="https://www.linkedin.com/in/maya-okawa">Maya Okawa</a>,
              <a href="https://rahulramesh.info">Rahul Ramesh</a>,
              <a href="https://mikailkhona.github.io">Mikail Khona</a>,
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>, and
              <strong>Ekdeep Singh Lubana</strong>
              <br>
              <em>International Conference on Machine Learning (ICML)</em>, 2025
              <br> 
<!--               <a href="https://arxiv.org/abs/2410.17194">arXiv</a> -->
              <p>We instantiate a synthetic knowledge graph domain to study how model editing protocols harm broader capabilities, demonstrating all representational organization of different concepts is destroyed under counterfactul edits.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/conceptspace.png" alt="Concept Spaces" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2406.19370">
                <papertitle>Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space</papertitle>
              </a>
              <br>
              <a href="https://cfpark00.github.io/">Core Francisco Park*</a>,
              <a href="https://www.linkedin.com/in/maya-okawa">Maya Okawa*</a>,
              <a href="https://ajyl.github.io/about">Andrew Lee</a>,
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>, and
              <strong>Ekdeep Singh Lubana</strong>
              <br>
              <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2024 <font color="red"> (Spotlight)</font> 
              <br> 
<!--               <a href="data/conceptspace.bib">bibtex</a> / <a href="https://arxiv.org/abs/2406.19370">arXiv</a> -->
              <p>We analyze a model's learning dynamics in "concept space" and identify sudden transitions where the model, when latently intervened, demonstrates a capability, even if input prompting does not show said capability.</p>
            </td>
          </tr>

          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sftjailbreaks.png" alt="SFT and Jailbreaks" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2407.10264">
                <papertitle>What Makes and Breaks Safety Fine-tuning? A Mechanistic Study</papertitle>
              </a>
              <br>
              <a href="https://samyakjain0112.github.io/">Samyak Jain</a>,
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="https://openreview.net/profile?id=~Kemal_Oksuz1">Kemal Oksuz</a>,
              <a href="https://thwjoy.github.io">Tom Joy</a>,
              <a href="https://www.robots.ox.ac.uk/~phst/">Philip H.S. Torr</a>,
              <a href="https://amartya18x.github.io/">Amartya Sanyal</a>, and
              <a href="https://puneetkdokania.github.io">Puneet K. Dokania</a>
              <br>
              <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2024
              <br>
              <em>ICML workshop on Mechanistic Interpretability </em>, 2024 <font color="red"> (Spotlight)</font> 
              <br> 
<!--               <a href="data/sftjailbreaks.bib">bibtex</a> / <a href="https://arxiv.org/abs/2407.10264">arXiv</a> -->
              <p>We use formal languages as a model system to identify the mechanistic changes induced by safety fine-tuning, and how jailbreaks bypass said mechanisms, verifying our claims on Llama models.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/matcomp.png" alt="Structure acquisition" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2410.22244">
                <papertitle>Abrupt Learning in Transformers: A Case Study on Matrix Completion</papertitle>
              </a>
              <br>
              <a href="https://pulkitgopalani.github.io">Pulkit Gopalani</a>,
              <strong>Ekdeep Singh Lubana</strong>, and
              <a href="http://weihu.me">Wei Hu</a>
              <br>
              <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2024
              <br> 
<!--               <a href="data/matcomp.bib">bibtex</a> / <a href="arxiv.org">arXiv (coming soon)</a> -->
              <p>We show the acquisition of structures underlying a data-generating process is the driving cause for abrupt learning in Transformers.</p>
            </td>
          </tr>

          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/challenges.png" alt="Challenges in LLMs' assurance" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://llm-safety-challenges.github.io">
                <papertitle>Foundational Challenges in Assuring Alignment and Safety of Large Language Models</papertitle>
              </a>
              <br>
              <a href="https://uzman-anwar.github.io">Usman Anwar</a>,
              <a href="https://asaparov.org">Abulhair Saparov<sup>*</sup></a>,
              <a href="https://javirando.com">Javier Rando<sup>*</sup></a>,
              <a href="https://danielpaleka.com">Daniel Paleka<sup>*</sup></a>,
              <a href="https://www.milesturp.in">Miles Turpin<sup>*</sup></a>,
              <a href="https://peterbhase.github.io">Peter Hase<sup>*</sup></a>, 
              <strong>Ekdeep Singh Lubana<sup>*</sup></strong>,
              <a href="https://ejenner.com">Erik Jenner<sup>*</sup></a>, 
              <a href="https://stephencasper.com">Stephen Casper<sup>*</sup></a>, 
              <a href="https://www.oliversourbut.net">Oliver Sourbut<sup>*</sup></a>, 
              <a href="https://www.benjaminedelman.com">Benjamin Edelman<sup>*</sup></a>,
              <a href="https://zowiezhang.github.io">Zhaowei Zhang<sup>*</sup></a>,
              <a href="https://www.mario-guenther.com">Mario Gunther<sup>*</sup></a>,
              <a href="https://www.korinek.com">Anton Korinek<sup>*</sup></a>,
              <a href="http://josephorallo.webs.upv.es">Jose Hernandez-Orallo<sup>*</sup></a>, and others
              <br>
              <em>Transactions on Machine Learning Research (TMLR) </em>, 2024
              <br> 
<!--               <a href="data/challenges.bib">bibtex</a> / <a href="https://arxiv.org/abs/2404.09932">arXiv</a> / <a href="https://llm-safety-challenges.github.io">website</a> -->
              <p>We identify and discuss 18 foundational challenges in assuring the alignment and safety of large language models (LLMs) and pose 200+ concrete research questions.</p>
            </td>
          </tr>
          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/capabilities.png" alt="Explosion of capabilities" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2311.12997">
                <papertitle>Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks</papertitle>
              </a>
              <br>
              <a href="https://rahulramesh.info">Rahul Ramesh</a>,
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="https://mikailkhona.github.io">Mikail Khona</a>,
              <a href="http://robertdick.org/">Robert P. Dick</a>, and
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>
              <br>
              <em>International Conference on Machine Learning (ICML)</em>, 2024
              <br> 
<!--               <a href="data/capabilities.bib">bibtex</a> / <a href="https://arxiv.org/abs/2311.12997">arXiv</a> -->
              <p>We formalize and define a notion of composition of primitive capabilities learned via autoregressive modeling by a Transformer, showing the model's capabilities can "explode", i.e., combinatorially increase if it can compose.</p>
            </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/stepwise.png" alt="Understanding stepwise inference" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2402.07757"> 
                <papertitle>Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model</papertitle>
              </a>
              <br>
              <a href="https://mikailkhona.github.io">Mikail Khona</a>,
              <a href="https://www.linkedin.com/in/maya-okawa">Maya Okawa</a>,
              <a href="https://janhula.com">Jan Hula</a>,
              <a href="https://rahulramesh.info">Rahul Ramesh</a>,
              <a href="https://kentonishi.com">Kento Nishi</a>,
              <a href="http://robertdick.org/">Robert P. Dick</a>,
              <strong>Ekdeep Singh Lubana<sup>*</sup></strong>, and
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka<sup>*</sup></a>
              <br>
              <em>International Conference on Machine Learning (ICML)</em>, 2024
              <br> 
<!--               <a href="data/stepwise.bib">bibtex</a> / <a href="https://arxiv.org/abs/2402.07757">arXiv</a> -->
              <p>We cast stepwise inference methods in LLMs as a graph navigation task, finding a synthetic model is sufficient to explain and identify novel characteristics of such methods.</p>
            </td>
          </tr>
          
          
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mft.png" alt="Mechanistic fine-tuning" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2311.12786">
                <papertitle>Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks</papertitle>
              </a>
              <br>
              <a href="https://samyakjain0112.github.io">Samyak Jain<sup>*</sup></a>,
              <a href="https://robertkirk.github.io">Robert Kirk<sup>*</sup></a>,
              <strong>Ekdeep Singh Lubana<sup>*</sup></strong>,
              <a href="http://robertdick.org/">Robert P. Dick</a>,
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>,
              <a href="https://www.egrefen.com">Edward Grefenstette</a>,
              <a href="https://rockt.github.io">Tim Rocktaschel</a>, and
              <a href="https://www.davidscottkrueger.com">David Krueger</a>
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2024
              <br> 
<!--               <a href="data/mft.bib">bibtex</a> / <a href="https://arxiv.org/abs/2311.12786">arXiv</a> -->
              <p>We show fine-tuning leads to learning of minimal transformations of a pretrained model's capabilities, like a "wrapper", by using procedural tasks defined using Tracr, PCFGs, and TinyStories.</p>
            </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/coinflipping.png" alt="GPT flips coins" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2310.17639">
                <papertitle>In-Context Learning Dynamics with Random Binary Sequences</papertitle>
              </a>
              <br>
              <a href="https://psychology.fas.harvard.edu/people/eric-bigelow">Eric J. Bigelow</a>,
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="http://robertdick.org/">Robert P. Dick</a>,
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>, and
              <a href="http://www.tomerullman.org">Tomer D. Ullman</a>
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2024
              <br> 
<!--               <a href="data/coinflipping.bib">bibtex</a> / <a href="https://arxiv.org/abs/2310.17639">arXiv</a> -->
              <p>We analyze different LLMs' abilities to model binary sequences generated via different pseduo-random processes, such as a formal automaton, and find that with scale, LLMs are (almost) able to simulate these processes via mere context conditioning.</p>
            </td>
          </tr>
          
          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/fomo.png" alt="GPT flips coins" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openreview.net/forum?id=nQp1FURyua">
                <papertitle>FoMo Rewards: Can we cast foundation models as reward functions? </papertitle>
              </a>
              <br>
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="https://johannbrehmer.github.io">Johann Brehmer</a>,
              <a href="http://pimdehaan.com">Pim de Haan</a>, and
              <a href="https://tacocohen.wordpress.com">Taco Cohen</a>
              <br>
              <em>NeurIPS workshop on Foundation Models for Decision Making</em>
              <br> 
<!--               <a href="data/fomo.bib">bibtex</a> / <a href="https://arxiv.org/abs/2312.03881">arXiv</a> -->
              <p>We propose and analyze a pipeline for re-casting an LLM as a generic reward function that interacts with an LVM to enable embodied AI tasks.</p>
            </td>
          </tr>
          
          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/concepts.png" alt="multiplicative emergence" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2310.09336">
                <papertitle>Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/maya-okawa">Maya Okawa<sup>*</sup></a>,
              <strong>Ekdeep Singh Lubana<sup>*</sup></strong>,
              <a href="http://robertdick.org/">Robert P. Dick</a>, and
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka<sup>*</sup></a>
              <br>
              <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2023
              <br> 
<!--               <a href="data/concepts.bib">bibtex</a> / <a href="https://arxiv.org/abs/2310.09336">arXiv</a> -->
              <p>We analyze compositionality in diffusion models, showing that there is a sudden emergence of this capability if models are allowed sufficient training to learn the relevant primitive capabilities.</p>
            </td>
          </tr>
          
          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mmc.png" alt="ssl_landscape" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2211.08422">
                <papertitle>Mechanistic Mode Connectivity</papertitle>
              </a>
              <br>
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="https://psychology.fas.harvard.edu/people/eric-bigelow">Eric J. Bigelow</a>,
              <a href="http://robertdick.org/">Robert P. Dick</a>,
              <a href="https://www.davidscottkrueger.com">David Krueger</a>, and
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>
              <br>
              <em>International Conference on Machine Learning (ICML)</em>, 2023
              <br> 
<!--               <a href="data/mmc.bib">bibtex</a> / <a href="https://arxiv.org/abs/2211.08422">arXiv</a> / <a href="https://github.com/EkdeepSLubana/MMC"> github </a> -->
              <p>We show models that rely on entirely different mechanisms for making their predictions can exhibit mode connectivity, but generally the ones that are mechanistically similar are linearly connected.</p>
            </td>
          </tr>
          
          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ssl_landscape.png" alt="ssl_landscape" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2210.00638.pdf">
                <papertitle>What Shapes the Landscape of Self-Supervised Learning?</papertitle>
              </a>
              <br>
              <a href="http://cat.phys.s.u-tokyo.ac.jp/~zliu/">Liu Ziyin</a>, 
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="http://cat.phys.s.u-tokyo.ac.jp/~ueda/E_index.html">Masahito Ueda</a>, and
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2023
              <br> 
<!--               <a href="data/ssl_landscape.bib">bibtex</a> / <a href="https://arxiv.org/pdf/2210.00638.pdf">arXiv</a>  -->
              <p>We present a highly detailed analysis of the landscape of several self-supervised learning objectives to clarify the role of representational collapse.</p>
            </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/graphssl.png" alt="GraphSSL" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2208.02810">
                <papertitle>Analyzing Data-Centric Properties for Contrastive Learning on Graphs</papertitle> 
              </a>
              <br>
              <a href="https://pujacomputes.github.io">Puja Trivedi</a>,
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="https://markheimann.github.io">Mark Heimann</a>,
              <a href="https://web.eecs.umich.edu/~dkoutra/">Danai Koutra</a>, and
              <a href="https://jjthiagarajan.com">Jay Jayaraman Thiagarajan</a>
              <br>
              <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2022
              <br>
<!--               <a href="data/graphssl.bib">bibtex</a> / <a href="https://arxiv.org/abs/2208.02810">arXiv</a> / <a href="https://github.com/pujacomputes/datapropsgraphSSL">github</a> -->
              <p>We propose a theoretical framework that demonstrates limitations of popular graph augmentation strategies for self-supervised learning.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/orchestra.png" alt="Orchestra" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2205.11506">
                <papertitle>Orchestra: Unsupervised Federated Learning via Globally Consistent Clustering</papertitle> 
              </a>
              <br>
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="https://iantangc.github.io">Chi Ian Tang</a>,
              <a href="https://www.fahim-kawsar.net">Fahim Kawsar</a>,
              <a href="http://robertdick.org/">Robert P. Dick</a>, and
              <a href="http://akhilmathurs.github.io">Akhil Mathur</a>
              <br>
              <em>International Conference on Machine Learning (ICML)</em>, 2022 <font color="red"> (Spotlight)</font> 
              <br>
<!--               <a href="data/orchestra.bib">bibtex</a> / <a href="https://arxiv.org/abs/2205.11506">arXiv</a> / <a href="https://github.com/akhilmathurs/orchestra">github</a> / <a href="https://slideslive.com/38984080/orchestra-unsupervised-federated-learning-via-globally-consistent-clustering">video</a>  -->
              <p>We propose an unsupervised learning method that exploits client heterogeneity to enable privacy preserving, SOTA performance unsupervised federated learning.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/beyondbn.png" alt="beyondbn" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2106.05956.pdf">
                <papertitle>Beyond BatchNorm: Towards a General Understanding of Normalization in Deep Learning</papertitle>
              </a>
              <br>
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>, and
              <a href="http://robertdick.org/">Robert P. Dick</a>
              <br>
              <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021
              <br>
<!--               <a href="data/beyondbn.bib">bibtex</a> / <a href="https://github.com/EkdeepSLubana/BeyondBatchNorm">github</a> / <a href="https://arxiv.org/abs/2106.05956">arXiv</a> / <a href="https://slideslive.com/38969102/beyond-batchnorm-towards-a-unified-understanding-of-normalization-in-deep-learning?ref=recommended">video</a>  -->
              <p>We develop a general theory to understand the role of normalization layers in improving training dynamics of a neural network at initialization.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/collas.png" alt="quadreg" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2102.02805.pdf">
                <papertitle>How do Quadratic Regularizers Prevent Catastrophic Forgetting: The Role of Interpolation</papertitle>
              </a>
              <br>
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="http://www-personal.umich.edu/~pujat/">Puja Trivedi</a>,
              <a href="https://web.eecs.umich.edu/~dkoutra/">Danai Koutra</a>, and
              <a href="http://robertdick.org/">Robert P. Dick</a>
              <br>
              <em>Conference on Lifelong Learning Agents (CoLLAs)</em>, 2022
              <br>
<!--               <a href="data/emr.bib">bibtex</a> / <a href="https://github.com/EkdeepSLubana/QRforgetting">github</a> / <a href="https://arxiv.org/abs/2102.02805">arXiv</a> / <a href="https://www.youtube.com/watch?v=gd5YzEnULHU">video</a>  -->
              <br>
              (Also presented at ICML Workshop on Theory and Foundations of Continual Learning, 2021) 
              <br>
              <p>This work demonstrates how quadratic regularization methods for preventing catastrophic forgetting in deep networks rely on a simple heuristic under-the-hood: Interpolation.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gradflow.png" alt="gradflow" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openreview.net/forum?id=rumv7QmLUue">
                <papertitle>A Gradient Flow Framework For Analyzing Network Pruning</papertitle>
              </a>
              <br>
              <strong>Ekdeep Singh Lubana</strong> and
              <a href="http://robertdick.org/">Robert P. Dick</a>
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2021 <font color="red"> (Spotlight)</font> 
              <br>
<!--               <a href="data/iclr.bib">bibtex</a> / <a href="https://github.com/EkdeepSLubana/flowandprune">github</a> / <a href="https://arxiv.org/abs/2009.11839">arXiv</a> / <a href="https://slideslive.com/38953851/a-gradient-flow-framework-for-analyzing-network-pruning">video</a>  -->
              <p>A unified, theoretically-grounded framework for network pruning that helps justify often used heuristics in the field.</p>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Undergraduate Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/minsip.png" alt="minsip" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/1HXnGcctlEI96S1edBIHjS88q5WM9a0h9/view?usp=sharing">
                <papertitle>Minimalistic Image Signal Processing for Deep Learning Applications</papertitle>
              </a>
              <br>
              <strong>Ekdeep Singh Lubana</strong>,
              <a href="http://robertdick.org/">Robert P. Dick</a>,
              Vinayak Aggarwal, 
              <a href="https://www.iitr.ac.in/departments/ECE/pages/People+Faculty+Pyari_Mohan_Pradhan.html">Pyari Mohan Pradhan</a>
              <br>
              <em>International Conference on Image Processing (ICIP)</em>, 2019
              <br>
<!--               <a href="data/icip.bib">bibtex</a> / -->
              <p>An image signal processing pipeline that allows use of out-of-the-box deep neural networks on RAW images directly retrieved from image sensors.</p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/foveation.jpg" alt="Digital Foveation" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/1WauNRLMVdzOEtlH4ZfawDAyBQiyV0p3I/view?usp=sharing">
                <papertitle>Digital Foveation: An Energy-Aware Machine Vision Framework</papertitle>
              </a>
              <br>
              <strong>Ekdeep Singh Lubana</strong> and
              <a href="http://robertdick.org/">Robert P. Dick</a>
              <br>
              <em>IEEE Transactions on Computer-Aided Design of Integrated Circuits and System (TCAD)</em>, 2018
              <br>
<!--               <a href="data/foveation.bib">bibtex</a> / -->
              <p>An energy-efficient machine vision framework inspired by the concept of Fovea in biological vision. Also see <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w40/Simpson_Intelligent_Scene_Caching_to_Improve_Accuracy_for_Energy-Constrained_Embedded_Vision_CVPRW_2020_paper.pdf">follow-up</a> work presented at CVPR workshop, 2020.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/snap.png" alt="SNAP" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/1JGDjSyoXzOqPSNYTZ4aQjZJQjQVkhYpn/view?usp=sharing">
                <papertitle>Snap: Chlorophyll Concentration Calculator Using RAW Images of Leaves</papertitle>
              </a>
              <br>
              <strong>Ekdeep Singh Lubana</strong>,
              Mangesh Gurav, and
              <a href="https://www.ee.iitb.ac.in/web/people/faculty/home/mshojaei">Maryam Shojaei Baghini</a>
              <br>
              <em>IEEE Sensors</em>, 2018;
              <em> Global Winner, Ericsson Innovation Awards</em> 2017
              <br>
<!--               <a href="data/snap.bib">bibtex</a> / <a href="https://www.ericsson.com/en/blog/2017/6/ericsson-innovation-awards-2017-working-together-for-the-future-of-food">news</a> -->
              <p>An efficient imaging system that accurately calculates chlorophyll content in leaves by using RAW images.</p>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website template source available <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
